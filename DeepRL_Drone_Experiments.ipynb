{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Trying to train a reinforcement learning agent to maximize area coverage by a swarm of drones"]},{"cell_type":"markdown","metadata":{},"source":["# --------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["# --------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the environment"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T04:30:50.078237Z","iopub.status.busy":"2023-04-19T04:30:50.077817Z","iopub.status.idle":"2023-04-19T04:30:50.097085Z","shell.execute_reply":"2023-04-19T04:30:50.096001Z","shell.execute_reply.started":"2023-04-19T04:30:50.078201Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","from collections import deque\n","\n","class Swarm:\n","    def __init__(self, n_drones, grid_size, init_state):\n","        self.n_drones = n_drones\n","        self.positions = []\n","        self.state = init_state\n","        pos = 0\n","        for i in range(len(init_state[0])):\n","            if init_state[0][i] == 2:\n","                # row = i // grid_size[0]\n","                # col = i % grid_size[0]\n","                self.positions.append(i)\n","        self.grid_size = grid_size\n","        self.state_size = self.grid_size[0] * self.grid_size[1]\n","        self.action_size = self.n_drones\n","        self.coverage_area = set()\n","\n","    def step(self, actions):\n","        rewards = torch.zeros(self.n_drones)\n","        \n","        cur_state = self.state.clone()\n","        # print(\"Taking step:\")\n","        for i, action in enumerate(actions):\n","            # action = torch.argmax(act, dim=1)\n","            pos = self.positions[i]\n","            row = pos // self.grid_size[1]\n","            col = pos % self.grid_size[1]\n","            # print(self.state[0][pos], pos, self.positions)\n","            cur_state[0][pos] = max(cur_state[0][pos] - 2, 1)  # covered\n","            row_old, col_old = row, col\n","            if action == 0: # move up\n","                row = max(0, row - 1)\n","            elif action == 1: # move down\n","                row = min(self.grid_size[0] - 1, row + 1)\n","            elif action == 2: # move left\n","                col = max(0, col - 1)\n","            elif action == 3: # move right\n","                col = min(self.grid_size[1] - 1, col + 1)\n","            self.positions[i] = row*self.grid_size[1] + col\n","            if cur_state[0][self.positions[i]] == 1:\n","                cur_state[0][self.positions[i]] = 2\n","            else:\n","                cur_state[0][self.positions[i]] += 2\n","            if self.positions[i] not in self.coverage_area:\n","                self.coverage_area.add(self.positions[i])\n","                rewards[i] = 1\n","            if row == row_old and col == col_old:\n","                rewards[i] = -1\n","                # print(f\"Unchanged {row}, {col}, Action: {action}\")\n","                # print(row*self.grid_size[1] + col)\n","        self.state = cur_state\n","        done = len(self.coverage_area) == self.state_size\n","        return rewards, done\n","    \n","    def get_state(self):\n","        return self.state\n","    \n","    def reset(self, init_state):\n","        self.state = init_state\n","        self.coverage_area = set()\n","        self.positions = []\n","        pos = 0\n","        for i in range(len(init_state[0])):\n","            if init_state[0][i] == 2:\n","                # row = i // grid_size[0]\n","                # col = i % grid_size[0]\n","                self.positions.append(i)\n"]},{"cell_type":"markdown","metadata":{},"source":["## The policy network (we're using on-policy learning)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T04:30:50.413576Z","iopub.status.busy":"2023-04-19T04:30:50.412791Z","iopub.status.idle":"2023-04-19T04:30:50.427263Z","shell.execute_reply":"2023-04-19T04:30:50.425778Z","shell.execute_reply.started":"2023-04-19T04:30:50.413531Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, n_drones, state_size, action_size):\n","        super().__init__()\n","        self.n_drones = n_drones\n","        self.x1 = nn.Linear(state_size, 64)\n","        self.x2 = nn.Linear(64, 32)\n","        self.x3 = nn.Linear(32, 32)\n","        self.x4 = nn.Linear(32, action_size)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax()\n","        \n","    def forward(self, state):\n","        op = self.x1(state)\n","        op = self.relu(op)\n","        op = self.x2(op)\n","        op = self.relu(op)\n","        op = self.x3(op)\n","        op = self.relu(op)\n","        op = self.x4(op)\n","        op = self.softmax(op)\n","        outputs = []\n","        for i in range(self.n_drones):\n","            outputs.append(op)\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["## The agent"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T04:30:50.769490Z","iopub.status.busy":"2023-04-19T04:30:50.769076Z","iopub.status.idle":"2023-04-19T04:30:50.810281Z","shell.execute_reply":"2023-04-19T04:30:50.808797Z","shell.execute_reply.started":"2023-04-19T04:30:50.769456Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","all_states = []\n","\n","class DQNAgent:\n","    def __init__(self, n_drones, state_size, action_size, model, learning_rate=0.001, discount_factor=0.95, batch_size=32, memory_size=10000):\n","        self.n_drones = n_drones\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.learning_rate = learning_rate\n","        self.discount_factor = discount_factor\n","        self.batch_size = batch_size\n","        self.memory = deque(maxlen=memory_size)\n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.999\n","        self.epsilon_min = 0.01\n","        # self.model = self.build_model()\n","        self.model = model\n","        \n","\n","    def build_model(self):\n","        \n","        model = tf.keras.Sequential()\n","        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n","        model.add(Dense(32, activation='relu'))\n","        model.add(Dense(self.action_size * self.n_drones, activation='linear'))\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        act_values = self.model(state)\n","        # if np.random.rand() <= self.epsilon:\n","        #    return\n","        act_values = [Categorical(act) for act in act_values]\n","        \n","        return act_values\n","    \n","    def plot_state(self, state, nrows, ncols):\n","        for i in range(nrows):\n","            for j in range(ncols):\n","                print(f\"{state[0][i*ncols+j]}\", end=\" | \")\n","            print(\"\\n\" + \"-\"*60)\n","        print(\"Pure state: \", state)\n","        print(\"\\n\\n\\n\\n\\n\")\n","\n","    def replay(self):\n","        if len(self.memory) < self.batch_size:\n","            return\n","        minibatch = random.sample(self.memory, self.batch_size)\n","        states = np.zeros((self.batch_size, self.state_size))\n","        actions = np.zeros((self.batch_size, self.n_drones), dtype=int)\n","        rewards = np.zeros((self.batch_size, self.n_drones))\n","        next_states = np.zeros((self.batch_size, self.state_size))\n","        dones = np.zeros((self.batch_size,))\n","        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n","            states[i] = state\n","            actions[i] = action\n","            rewards[i] = reward\n","            next_states[i] = next_state\n","            dones[i] = done\n","            \n","        \n","        targets = self.model.predict(states)\n","        q_next = self.model.predict(next_states)\n","        for i in range(self.batch_size):\n","            for j in range(self.n_drones):\n","                if dones[i]:\n","                    targets[i][j] = rewards[i][j]\n","                else:\n","                    targets[i][j] = rewards[i][j] + self.discount_factor * np.max(q_next[i][j])\n","                    print(f\"Done with {i}, {j}\")\n","        self.model.fit(states, targets, epochs=1, verbose=0)\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","    def learn(self, env, n_episodes):\n","        torch.autograd.set_detect_anomaly(True)\n","        optimizer = optim.Adam(model.parameters(), lr=0.01)\n","        for i_episode in range(n_episodes):\n","            state = torch.zeros((1, self.state_size))\n","            init_indices = np.random.choice(len(state[0]), size=self.n_drones, replace=False)\n","            state[0][init_indices] = 2  # initial positions of the drones\n","            # state = state  # creates a new state object and points it to the \n","            env.reset(state)\n","            done = False\n","            total_reward = 0\n","            timestep = 0\n","            all_states.append([])\n","            while not done and timestep < 1000:\n","                actions = self.act(env.get_state())\n","                # act_values = [torch.argmax(ac, dim=1) for ac in actions]\n","                act_values = [act.sample() for act in actions]\n","                rewards, done = env.step(act_values)\n","                \n","                loss = [actions[i].log_prob(act_values[i])*(rewards[i]+ (1e-2)) for i in range(self.n_drones)]\n","                loss = -1 * sum(loss)\n","                \n","                # filter the logits, those that gave reward 1 should prevail. others set to 0\n","                # loss = [actions[i] * abs(1 - rewards[i]) for i in range(self.n_drones)]\n","                # loss = torch.stack(loss)\n","                # print(f\"Loss shape: {loss.shape}\\nActions shape: {actions[0].shape}\")\n","                # loss = torch.sum(loss, dim=2)\n","                # loss = torch.sum(loss, dim=0)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","                \n","                total_reward = len(env.coverage_area)\n","                \n","                # state = next_state\n","                if (timestep % 100 == 0 or timestep % 100 == 1):\n","                    print(f\"Timestep: {timestep}, Rewards: {total_reward}\")\n","                    self.plot_state(env.get_state(), 10, 10)\n","                    \n","                all_states[-1].append(np.reshape(env.get_state(), env.grid_size))\n","                timestep += 1\n","                writer.add_scalar(f'Reward progression {i_episode}', total_reward, timestep)\n","                # next_state = np.array([len(env.coverage_area) / (env.grid_size[0] * env.grid_size[1])])\n","                # self.remember(state, actions, rewards, next_state, done)\n","                # self.replay()\n","            print(\"Episode {}/{}, Total Reward: {}, Epsilon: {:.2}, Timesteps: {}\"\n","                  .format(i_episode + 1, n_episodes, total_reward, self.epsilon, timestep))\n","            self.plot_state(env.get_state(), 10, 10)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Let it rip"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T04:30:51.145184Z","iopub.status.busy":"2023-04-19T04:30:51.144749Z","iopub.status.idle":"2023-04-19T04:33:06.964090Z","shell.execute_reply":"2023-04-19T04:33:06.962200Z","shell.execute_reply.started":"2023-04-19T04:30:51.145139Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"name":"stdout","output_type":"stream","text":["Timestep: 0, Rewards: 10\n","0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 1.0 | 2.0 | 1.0 | 2.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 2., 0., 0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 2., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 1., 2., 0., 2., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 1, Rewards: 17\n","0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 2., 0., 0., 0., 0., 2., 1., 0., 0., 0., 1., 0., 0., 0., 0., 2., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n","         2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 1., 0., 0., 0., 0., 0., 0.,\n","         0., 2., 0., 0., 0., 2., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n","         0., 0., 0., 0., 0., 2., 0., 1., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Episode 1/100, Total Reward: 100, Epsilon: 1.0, Timesteps: 89\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 4.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | \n","------------------------------------------------------------\n","1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1.,\n","         1., 1., 1., 1., 1., 1., 4., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 2., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 0, Rewards: 10\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 2.0 | 1.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 2., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 2., 0., 0., 2., 1., 2., 0., 0., 0., 0.,\n","         1., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0.,\n","         2., 0., 0., 2., 0., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 1, Rewards: 19\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 2.0 | 2.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 1.0 | 2.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         2., 0., 1., 2., 2., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n","         1., 2., 1., 2., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n","         2., 0., 2., 1., 0., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Episode 2/100, Total Reward: 100, Epsilon: 1.0, Timesteps: 74\n","2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 2.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n","         1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 1.,\n","         1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n","         1., 1., 1., 2., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 0, Rewards: 10\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 2., 0., 0., 0., 0., 2., 0.,\n","         0., 0., 1., 0., 0., 0., 2., 1., 0., 0., 0., 2., 2., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 1, Rewards: 20\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 2.0 | 0.0 | 2.0 | 1.0 | 2.0 | 0.0 | 0.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 2., 0., 0., 0., 0., 0., 0., 2.,\n","         2., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 2.,\n","         0., 2., 1., 2., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 2., 2.,\n","         0., 0., 0., 0., 2., 0., 0., 0., 0., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 100, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 101, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 200, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 201, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 300, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 301, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 400, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 401, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 500, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 501, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 600, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 601, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 700, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 701, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 800, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 801, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 900, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 901, Rewards: 89\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Episode 3/100, Total Reward: 89, Epsilon: 1.0, Timesteps: 1000\n","6.0 | 0.0 | 4.0 | 2.0 | 1.0 | 4.0 | 2.0 | 0.0 | 2.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[6., 0., 4., 2., 1., 4., 2., 0., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n","         1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 0, Rewards: 9\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 2.0 | 0.0 | 2.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","         2., 0., 0., 0., 2., 2., 0., 2., 0., 2., 1., 0., 0., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 1, Rewards: 17\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 2.0 | 0.0 | 2.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 2., 0., 0., 0., 0., 1., 0., 0.,\n","         0., 0., 1., 0., 0., 0., 0., 0., 2., 0., 0., 0., 2., 2., 0., 2., 0., 2.,\n","         1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 100, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 101, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 200, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 201, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 300, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 301, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 400, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 401, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 500, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 501, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 600, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 601, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 700, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 701, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 800, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 801, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 900, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 901, Rewards: 41\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Episode 4/100, Total Reward: 41, Epsilon: 1.0, Timesteps: 1000\n","4.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 2.0 | 2.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 2., 0., 2., 0., 6., 2., 2., 2., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n","         1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n","         1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n","         0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 0, Rewards: 10\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 2., 0., 0., 0., 2., 0., 2., 0., 2., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 1, Rewards: 17\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","2.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[0., 0., 0., 0., 0., 0., 2., 0., 0., 2., 2., 0., 0., 0., 0., 0., 1., 0.,\n","         0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 2., 0.,\n","         2., 0., 2., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 100, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 101, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 200, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 201, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 300, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 301, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 400, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n","Timestep: 401, Rewards: 44\n","4.0 | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 6.0 | 2.0 | 0.0 | 4.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | \n","------------------------------------------------------------\n","Pure state:  tensor([[4., 0., 2., 0., 2., 0., 6., 2., 0., 4., 1., 0., 1., 0., 1., 0., 1., 1.,\n","         0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n","         1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n","         1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n","         1., 0., 1., 0., 1., 0., 0., 0., 0., 0.]])\n","\n","\n","\n","\n","\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/4294128930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_drones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_drones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/3876134271.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, env, n_episodes)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_drones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3876134271.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_drones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_lazy_property_and_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlazy_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs_to_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlazy_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36mprobs_to_logits\u001b[0;34m(probs, is_binary)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mdenote\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0mof\u001b[0m \u001b[0moccurrence\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mps_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclamp_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps_clamped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mps_clamped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36mclamp_probs\u001b[0;34m(probs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclamp_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# First call the original checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["n_drones = 10\n","grid_size = (10, 10)\n","state_size = grid_size[0] * grid_size[1]\n","action_size = 4\n","state = torch.zeros((1, state_size))\n","init_indices = np.random.choice(len(state[0]), size=n_drones, replace=False)\n","state[0][init_indices] = 2  # initial positions of the drones\n","env = Swarm(n_drones=n_drones, grid_size=grid_size, init_state=state)\n","model = PolicyNetwork(n_drones, state_size=state_size, action_size=action_size)\n","agent = DQNAgent(n_drones=10, state_size=state_size, action_size=action_size, model=model)\n","agent.learn(env, n_episodes=100)"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T05:06:49.429203Z","iopub.status.busy":"2023-04-19T05:06:49.428758Z","iopub.status.idle":"2023-04-19T05:07:46.405622Z","shell.execute_reply":"2023-04-19T05:07:46.404166Z","shell.execute_reply.started":"2023-04-19T05:06:49.429160Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import seaborn as sns\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.pyplot as plt\n","fig, ax = plt.subplots(figsize=(10, 10))\n","def animate(data):\n","    ax = sns.heatmap(data, vmin=0, vmax=4, cbar=False)\n","    return ax\n","    \n","ani = FuncAnimation(fig, animate, frames=all_states[1], interval=100)\n","ani.save('myAnimation1.gif', writer='imagemagick', fps=10)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
